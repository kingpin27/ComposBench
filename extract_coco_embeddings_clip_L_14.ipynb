{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48bKAZoKSlEU",
        "outputId": "3a1f2c9b-c42a-41a4-9fdf-b993b3b18242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-19 20:56:36--  http://images.cocodataset.org/zips/unlabeled2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.51.57, 3.5.24.90, 16.15.176.39, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.51.57|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20126613414 (19G) [application/zip]\n",
            "Saving to: ‘unlabeled2017.zip’\n",
            "\n",
            "unlabeled2017.zip   100%[===================>]  18.74G  17.9MB/s    in 18m 49s \n",
            "\n",
            "2026-02-19 21:15:26 (17.0 MB/s) - ‘unlabeled2017.zip’ saved [20126613414/20126613414]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://images.cocodataset.org/zips/unlabeled2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip unlabeled2017.zip > /dev/null"
      ],
      "metadata": {
        "id": "MN13CFbISpZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch torchvision pandas numpy pillow tqdm open_clip_torch"
      ],
      "metadata": {
        "id": "sEzFEkSdebIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Generate CLIP ViT-L/14 embeddings for COCO unlabeled2017 images\n",
        "and save them to a Parquet file with file names.\n",
        "\n",
        "Usage:\n",
        "    python generate_clip_embeddings.py\n",
        "\n",
        "Requirements:\n",
        "    pip install torch torchvision open-clip-torch pandas pyarrow pillow tqdm\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import open_clip\n",
        "\n",
        "# ── Config ────────────────────────────────────────────────────────────────────\n",
        "IMAGE_DIR   = \"./unlabeled2017\"\n",
        "OUTPUT_FILE = \"./clip_embeddings.parquet\"\n",
        "BATCH_SIZE  = 64          # lower if you run out of VRAM\n",
        "MODEL_NAME  = \"ViT-L-14\"\n",
        "PRETRAINED  = \"openai\"    # uses OpenAI's original CLIP weights\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load model\n",
        "    print(f\"Loading {MODEL_NAME} ({PRETRAINED}) …\")\n",
        "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "        MODEL_NAME, pretrained=PRETRAINED, device=device\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    # Gather image paths\n",
        "    extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\")\n",
        "    image_paths = []\n",
        "    for ext in extensions:\n",
        "        image_paths.extend(glob.glob(os.path.join(IMAGE_DIR, ext)))\n",
        "    image_paths.sort()\n",
        "\n",
        "    if not image_paths:\n",
        "        raise FileNotFoundError(f\"No images found in '{IMAGE_DIR}'\")\n",
        "    print(f\"Found {len(image_paths):,} images\")\n",
        "\n",
        "    all_embeddings = []\n",
        "    all_filenames  = []\n",
        "    failed         = []\n",
        "\n",
        "    # Process in batches\n",
        "    for batch_start in tqdm(range(0, len(image_paths), BATCH_SIZE), desc=\"Encoding\"):\n",
        "        batch_paths = image_paths[batch_start : batch_start + BATCH_SIZE]\n",
        "\n",
        "        tensors   = []\n",
        "        filenames = []\n",
        "        for path in batch_paths:\n",
        "            try:\n",
        "                img = Image.open(path).convert(\"RGB\")\n",
        "                tensors.append(preprocess(img))\n",
        "                filenames.append(os.path.basename(path))\n",
        "            except Exception as e:\n",
        "                print(f\"\\n  ⚠  Skipping {path}: {e}\")\n",
        "                failed.append(path)\n",
        "\n",
        "        if not tensors:\n",
        "            continue\n",
        "\n",
        "        batch_tensor = torch.stack(tensors).to(device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
        "            features = model.encode_image(batch_tensor)\n",
        "            features = features / features.norm(dim=-1, keepdim=True)  # L2-normalise\n",
        "\n",
        "        all_embeddings.append(features.cpu().float().numpy())\n",
        "        all_filenames.extend(filenames)\n",
        "\n",
        "    embeddings_np = np.concatenate(all_embeddings, axis=0)  # (N, 768)\n",
        "\n",
        "    # Build DataFrame: one column for filename, one column holding the embedding array\n",
        "    df = pd.DataFrame({\n",
        "        \"filename\":  all_filenames,\n",
        "        \"embedding\": list(embeddings_np),   # each cell is a (768,) numpy array\n",
        "    })\n",
        "\n",
        "    df.to_parquet(OUTPUT_FILE, index=False)\n",
        "    print(f\"\\n✅  Saved {len(df):,} embeddings → {OUTPUT_FILE}\")\n",
        "    if failed:\n",
        "        print(f\"   ⚠  {len(failed)} images failed (see warnings above)\")\n",
        "\n",
        "    # Quick sanity check\n",
        "    df_check = pd.read_parquet(OUTPUT_FILE)\n",
        "    emb_shape = np.array(df_check[\"embedding\"].iloc[0]).shape\n",
        "    print(f\"   Parquet rows: {len(df_check):,} | Embedding dim: {emb_shape}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhT8XbrLc1MU",
        "outputId": "153454b9-d9e7-4228-ed91-a1a52cb41d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading ViT-L-14 (openai) …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 123,403 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEncoding:   0%|          | 0/1929 [00:00<?, ?it/s]/tmp/ipython-input-3649485187.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
            "Encoding: 100%|██████████| 1929/1929 [47:41<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅  Saved 123,403 embeddings → ./clip_embeddings.parquet\n",
            "   Parquet rows: 123,403 | Embedding dim: (768,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bTh8zGGeehVG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}